# Agentic RAG System for German Medical Product Catalog

A production-ready Streamlit application that uses agentic RAG (Retrieval-Augmented Generation) to answer questions about German medical products in English, with full source traceability.

## üéØ Problem Statement

Hospital procurement staff need to query German medical product catalogs in English, getting accurate answers with product IDs and source citations. This system bridges the language gap while maintaining precision and traceability.

## üèóÔ∏è Architecture
```
User Query (English)
    ‚Üì
Agent (Query Classification)
    ‚Üì
Adaptive Retrieval (FAISS + Sentence Transformers)
    ‚Üì
LLM Generation (Llama 3.2 via Ollama)
    ‚Üì
Answer (English) + Citations + Product IDs
```

## ü§ñ Agentic Behavior

The system implements intelligent routing based on query classification:

### Query Types:
1. **Product Lookup** (`top_k=3`)
   - Single product queries
   - Specific feature questions
   - Example: "What is Omnifix?"

2. **Comparison** (`top_k=8`)
   - Multi-product comparisons
   - Feature comparison queries
   - Example: "Compare Omnifix and Injekt syringes"

3. **Out of Scope**
   - Non-product queries
   - Requests for information not in catalog

### Why This Approach?

- **Lightweight**: Reduces latency and failure modes
- **Explainable**: Clear decision logic for debugging
- **Scalable**: Easy to extend with more sophisticated classifiers

## üöÄ Features

- ‚úÖ **Multilingual Support**: German source ‚Üí English answers
- ‚úÖ **Source Attribution**: Every answer cites page numbers and product IDs
- ‚úÖ **Agentic Routing**: Adaptive retrieval based on query type
- ‚úÖ **Local-First**: Runs entirely offline with Ollama
- ‚úÖ **Production Ready**: Logging, error handling, and observability

## üìÅ Project Structure
```
agentic-rag-sanovio/
‚îÇ
‚îú‚îÄ‚îÄ app.py                  # Streamlit interface
‚îú‚îÄ‚îÄ ingest.py               # PDF parsing and chunking
‚îú‚îÄ‚îÄ index.py                # Vector embedding and FAISS indexing
‚îú‚îÄ‚îÄ agent.py                # Query classification and routing logic
‚îú‚îÄ‚îÄ rag.py                  # Retrieval and generation pipeline
‚îú‚îÄ‚îÄ prompts.py              # Centralized prompt management
‚îú‚îÄ‚îÄ logger.py               # Observability and logging
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ product_catalog_01.pdf
‚îÇ
‚îú‚îÄ‚îÄ index_store/
‚îÇ   ‚îú‚îÄ‚îÄ faiss.index         # Vector index (generated)
‚îÇ   ‚îî‚îÄ‚îÄ metadata.pkl        # Chunk metadata (generated)
‚îÇ
‚îú‚îÄ‚îÄ .env.example            # Environment variables template
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

## üõ†Ô∏è Installation

### Prerequisites

- Python 3.8+
- Ollama installed locally

### Step 1: Clone the Repository
```bash
git clone https://github.com/YOUR_USERNAME/agentic-rag-sanovio.git
cd agentic-rag-sanovio
```

### Step 2: Install Ollama
```bash
# macOS
brew install ollama

# Start Ollama
ollama serve

# In a new terminal, pull the model
ollama pull llama3.2
```

### Step 3: Set Up Python Environment
```bash
# Create virtual environment
python -m venv venv

# Activate it
source venv/bin/activate  # On macOS/Linux
# venv\Scripts\activate   # On Windows

# Install dependencies
pip install -r requirements.txt
```

### Step 4: Prepare the Data
```bash
# Run ingestion
python ingest.py

# Build FAISS index
python index.py
```

### Step 5: Run the Application

Make sure Ollama is running (`ollama serve` in a separate terminal), then:
```bash
streamlit run app.py
```

The app will open at `http://localhost:8501`

## üìä Usage Examples

**Product Lookup:**
```
Q: What is Omnifix?
A: Omnifix is a three-piece disposable syringe with Luer-Lock connection...
   [Product ID: 9151101] (Page 4)
```

**Comparison:**
```
Q: Compare Omnifix and Injekt syringes
A: Based on the catalog:
   - Omnifix: Three-piece design with Luer-Lock (Product ID: 9151101)
   - Injekt: Two-piece design with Luer connection (Product ID: 4606710V0)
   ...
```
## üß™ Evaluation

The system includes an automated evaluation suite to measure quality across three dimensions:

### Running Evaluation
```bash
python evaluate.py
```

This will:
1. Run all test cases from `eval/eval_dataset.json`
2. Evaluate agent classification, retrieval recall, and answer quality
3. Save detailed results to `eval/eval_results.json`
4. Print a summary report

### Evaluation Metrics

**Agent Classification Accuracy**
- Measures if queries are correctly routed (product_lookup vs comparison vs out_of_scope)

**Retrieval Recall**
- Checks if expected product IDs appear in retrieved chunks
- Target: >85% recall for all queries

**Answer Quality (Keyword Coverage)**
- Verifies that answers contain expected domain terms
- Ensures answers are in English despite German source material

### Sample Output
```
Test Case 1: What is Omnifix?
üìä Agent Classification: ‚úì Correct (product_lookup)
üîç Retrieval Recall: 100% (Found: ['9151101'])
üí¨ Keyword Coverage: 100% (Found: ['syringe', 'Luer-Lock', 'three-piece'])
```

### Test Dataset

The evaluation suite includes:
- 2 product lookup queries
- 1 comparison query
- 1 out-of-scope query (to test refusal behavior)

To add more test cases, edit `eval/eval_dataset.json`.

### Limitations & Future Work

Current evaluation is **rule-based** (keyword matching). For production:
- [ ] Add LLM-as-judge for semantic evaluation
- [ ] Collect human ratings from domain experts
- [ ] Implement RAGAS or similar framework
- [ ] Track hallucination rate
- [ ] Monitor citation accuracy

## üîß Design Decisions & Trade-offs

### Chunking Strategy
- **Chunk Size**: 500-700 tokens
- **Overlap**: 100 tokens
- **Rationale**: Product specifications often span multiple lines; overlap ensures context preservation

### Embedding Model
- **Choice**: `sentence-transformers/all-MiniLM-L6-v2`
- **Why**: Good multilingual support, fast, runs locally
- **Trade-off**: Smaller model = faster but slightly less accurate than larger models

### LLM Choice
- **Choice**: Llama 3.2 via Ollama
- **Why**: Free, private, no API costs, offline capability
- **Trade-off**: Slightly less capable than GPT-4, but sufficient for this use case

### Agent Complexity
- **Choice**: Simple rule-based classifier
- **Why**: Reduces latency, easier to debug, fewer failure modes
- **Future**: Could upgrade to LLM-based classifier for more nuanced routing

## ‚ö†Ô∏è Limitations

1. **German Text Processing**: Currently basic; could improve with domain-specific preprocessing
2. **Citation Accuracy**: Relies on chunk boundaries; very long product specs may be split
3. **Query Understanding**: Simple keyword-based routing; complex queries might be misclassified
4. **Hallucination Risk**: Mitigated by strict prompts, but not eliminated

## üîÆ Future Improvements

- [ ] Add LLM-based query classifier for better routing
- [ ] Implement semantic caching for frequent queries
- [ ] Add support for multiple PDF catalogs
- [ ] Build evaluation suite with ground-truth Q&A pairs
- [ ] Add conversation memory for multi-turn interactions
- [ ] Implement hybrid search (keyword + semantic)

## üß™ Testing
```bash
# Run basic tests
python -m pytest tests/

# Test indexing
python index.py

# Test retrieval
python -c "from rag import retrieve; print(retrieve('What is Omnifix?'))"
```

## üìù Logging & Observability

All queries are logged with:
- Query text
- Agent decision
- Retrieved chunk IDs
- Response latency
- Timestamp

Logs are stored in `logs/` directory.

## ü§ù Contributing

This is a portfolio project, but feedback is welcome! Please open an issue or submit a PR.

## üìÑ License

MIT License - feel free to use this for learning or commercial purposes.

## üë§ Author

**Nikhil**
- GitHub: [@YOUR_USERNAME](https://github.com/YOUR_USERNAME)
- LinkedIn: [Your LinkedIn](https://linkedin.com/in/YOUR_PROFILE)

## üôè Acknowledgments

- Built as part of a technical challenge for [Company Name]
- Inspired by modern RAG architectures and agentic AI systems

---

**Note for Reviewers**: This project demonstrates:
- Production-quality code organization
- Thoughtful architectural decisions
- Clear documentation and rationale
- Understanding of RAG limitations and trade-offs
```

**1.3 Create `.env.example`**

Create a `.env.example` file (this goes to GitHub, unlike `.env`):
```
# This is an example environment file
# Copy this to .env and fill in your actual values

# Only needed if using OpenAI or Anthropic (optional)
# OPENAI_API_KEY=your-key-here
# ANTHROPIC_API_KEY=your-key-here

# Ollama runs locally and doesn't need API keys